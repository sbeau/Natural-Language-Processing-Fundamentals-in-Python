{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions & word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular expressions: re.split() and re.findall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_one = \"SCENE 1: [wind] [clop clop clop] KING ARTHUR: Whoa there!  [clop clop clop] SOLDIER #1: Halt!  Who goes there? ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'son', 'sovereign', 'England', 'clop', '[', 'KING', 'is', 'It', 'SOLDIER', 'I', 'King', 'defeator', 'of', 'there', 'ARTHUR', 'the', 'Pendragon', 'wind', ']', '#', 'Arthur', 'SCENE', 'Camelot', '!', 'all', '.', 'Whoa', 'castle', 'Who', '?', 'Uther', '1', 'Saxons', ',', ':', 'goes', 'from', 'Britons', 'Halt'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More regex with re.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 21\n",
      "<_sre.SRE_Match object; span=(9, 75), match='[wind] [clop clop clop] KING ARTHUR: Whoa there! >\n",
      "<_sre.SRE_Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"clop\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex with NLTK tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python', '#NLP is super fun! <3 #learning', 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern1)\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern2 = r\"([#@]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1], pattern2)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text = \"Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•']\n",
      "['Wann', 'Pizza', 'Und', 'Ãœber']\n",
      "['ðŸ•', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charting practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿      [wind]\n",
      "      [clop clop]\n",
      "  ARTHUR:  Whoa there!\n",
      "      [clop clop]\n",
      " \n",
      "  GUARD #1:  Halt!  Who goes there?\n",
      "  ARTHUR:  It is I, Arthur, son of Uther Pendragon, from the castle\n",
      "      of Camelot.  King of the Britons, defeator of the Saxons, sovereign\n",
      "      of all England!\n",
      "  GUARD #1:  Pull the other one!\n",
      "  ARTHUR:  I am.  And this my trusty servant Patsy.\n",
      "      We have ridden the length and breadth of the land in search of knights\n",
      "      who will join me in my court of Camelot.  I must speak with your lord\n",
      "      and master.\n",
      "  GUARD #1:  What, ridden on a horse?\n",
      "  ARTHUR:  Yes!\n",
      "  GUARD #1:  You're using coconuts!\n",
      "  ARTHUR:  What?\n",
      "  GUARD #1:  You've got two empty halves of coconut and you're bangin'\n",
      "      'em together.\n",
      "  ARTHUR:  So?  We have ridden since the snows of winter covered this\n",
      "      land, through the kingdom of Mercea, through--\n",
      "  GUARD #1:  Where'd you get the coconut?\n",
      "  ARTHUR:  We found them.\n",
      "  GUARD #1:  Found them?  In Mercea?  The coconut's tropical!\n",
      "  ARTHUR:  What do you mean?\n",
      "  GUARD #1:  Well, this is a temperate zone.\n",
      "  ARTHUR:  The swallow may fly south with the sun or the house martin\n",
      "      or the plumber may seek warmer climes in winter yet these are not\n",
      "      strangers to our land.\n",
      "  GUARD #1:  Are you suggesting coconuts migrate?\n",
      "  ARTHUR:  Not at all, they could be carried.\n",
      "  GUARD #1:  What -- a swallow carrying a coconut?\n",
      "  ARTHUR:  It could grip it by the husk!\n",
      "  GUARD #1:  It's not a question of where he grips it!  It's a simple\n",
      "      question of weight ratios!  A five ounce bird could not carry a 1 pound\n",
      "      coconut.\n",
      "  ARTHUR:  Well, it doesn't matter.  Will you go and tell your master\n",
      "      that Arthur from the Court of Camelot is here.\n",
      "  GUARD #1:  Listen, in order to maintain air-speed velocity, a swallow\n",
      "      needs to beat its wings 43 times every second, right?\n",
      "  ARTHUR:  Please!\n",
      "  GUARD #1:  Am I right?\n",
      "  ARTHUR:  I'm not interested!\n",
      "  GUARD #2:  It could be carried by an African swallow!\n",
      "  GUARD #1:  Oh, yeah, an African swallow maybe, but not a European\n",
      "      swallow, that's my point.\n",
      "  GUARD #2:  Oh, yeah, I agree with that...\n",
      "  ARTHUR:  Will you ask your master if he wants to join my court\n",
      "      at Camelot?!\n",
      "  GUARD #1:  But then of course African swallows are not migratory.\n",
      "  GUARD #2:  Oh, yeah...\n",
      "  GUARD #1:  So they couldn't bring a coconut back anyway...\n",
      "      [clop clop]\n",
      "  GUARD #2:  Wait a minute -- supposing two swallows carried it together?\n",
      "  GUARD #1:  No, they'd have to have it on a line.\n",
      "  GUARD #2:  Well, simple!  They'd just use a standard creeper!\n",
      "  GUARD #1:  What, held under the dorsal guiding feathers?\n",
      "  GUARD #2:  Well, why not?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('scene_one.txt', 'r') as f:\n",
    "    scene_one = f.read()\n",
    "print(scene_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = scene_one.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
